---
layout: page
title: Projects
subtitle: 
---

<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-175479624-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-175479624-1');
</script>
</head>
<h3>Deep Learning Models for NLP</h3>
**Deep Averaging Network and GRU:** Developed a [Deep Averaging Networks (DAN)](https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf) and [Gated Recurrent Units (GRU)](https://arxiv.org/pdf/1412.3555.pdf) to perform sentiment analysis. Evaluated the information learned at each layer using linear probes (linear classifiers) on the output at each layer.

**Neural-network based transition parsing:** Developed a neural-network based transition parsing (arc-standard algorithm) model with a custom cubic activation function. Studied the performance of cubic activation vs tanh and sigmoid activations. Reference papers used: [Incrementality in Deterministic Dependency Parsing](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.5472&rep=rep1&type=pdf) and [A Fast and Accurate Dependency Parser using Neural Networks](https://nlp.stanford.edu/pubs/emnlp2014-depparser.pdf)

**Bi-Directional GRU with Attention:** Developed a model using bi-directional GRU with attention to perform relation extraction on the SemEval 2010 dataset. Reference papers used: [SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations Between Pairs of Nominals](https://www.aclweb.org/anthology/S10-1006.pdf) and [Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification](https://www.aclweb.org/anthology/P16-2034.pdf)

Technologies Used: [TensorFlow 2.0](https://www.tensorflow.org/)
<hr />

<h3>Word2Vec - Skipgram and Bias Evaluation (WEAT)</h3>
Implemented the Word2Vec - Skipgram model using Cross Entropy (CE) and [Noise Contrastive Estimation (NCE)](https://www.cs.toronto.edu/~amnih/papers/wordreps.pdf) loss functions. 
- Studied the advantages of using NCE loss over Cross Entropy loss.
- Evaluated the bias introduced in word embeddings generated by this model using [Word Embedding Association Test (WEAT)](https://arxiv.org/pdf/1810.03611.pdf)

<hr />

<h3>Detecting Deceptive Hotel Reviews</h3>
Re-implementation of the paper [Finding Deceptive Opinion Spam by Any Stretch of the Imagination](https://www.aclweb.org/anthology/P11-1032/). Here, I developed machine learning models to classify honest and deceptive reviews for the top 20 hotels in Chicago. The hotel reviews were obtained from TripAdvisor.
- Dataset: The dataset consisted of 400 truthful and 400 deceptive hotel reviews.
- Algorithms used: Support Vector Machines, Naive Bayes
- Language encodings used: Uni-gram, Bi-gram, Tri-gram. The Bi-gram and Tri-gram representations consumed the prior n-gram notation as well.
- Technologies used: Python
- Libraries used: [Scikit-Learn](https://scikit-learn.org/stable/), [NLTK](https://www.nltk.org/)
- You may view the code for this project on GitHub [here](https://github.com/aakashsbhatia2/Deception-detection)

<hr /> 

<h3>Interactive Visual Dashboard for Road Accidents in the United States</h3>
Developed a concise, interactive, single-screen visual dashboard using Python and d3.js to study the effects of weather conditions on road accidents within the United States.
- Dataset: The dataset contained statistics of road accidents with the United States. This dataset was obtained from Kaggle. The dataset can be found [here](https://www.kaggle.com/sobhanmoosavi/us-accidents)
- Technologies used:
    - JavaScript: I used d3.js to create the visualisations
    - Python: A Flask Server to host the dashboard, perform operations on the data (Principal Component Analysis, k-means clustering), render updated data to the dashboard.
    - HTML, CSS: To create the webpage containing the visualisations
- Data Science Techniques used: 
    - Principal Component Analysis: I performed dimension reduction using PCA to generate a scatter-plot matrix of the top 2 PC vectors.
    - Stratified Sampling using k-means clustering: To project the data onto a parallel Co-ordinate chart, I performed stratified sampling using k-means clustering. This allowed me to reduce the number of datapoints being projected while maintaining the data distribution.
- Libraries Used: [Scikit-Learn](https://scikit-learn.org/stable/), [d3.js](https://d3js.org/), [Flask](https://pypi.org/project/Flask/)
- You may view the code for this project on GitHub [here](https://github.com/aakashsbhatia2/Visualization-Project)

<hr /> 

<h3>Machine Learning Algorithms from scratch </h3>
Implemented 5 machine learning algorithms from scratch (i.e. WITHOUT [Scikit-Learn](https://scikit-learn.org/stable/)). The algorithms I implemented are:
- Perceptron:
    - I implemented the perceptron algorithm from scratch using Python. My implementation can be found on GitHub [here](https://github.com/aakashsbhatia2/Perceptron-AdaBoost)
- Adaptive Boosting (AdaBoost):
    - I implemented Adaptive Boosting from scratch using Python. 
    - I used decision stumps as the weak learners for my implementation
    - My implementation can be found on GitHub [here](https://github.com/aakashsbhatia2/Perceptron-AdaBoost)
- Support Vector Machines (SVM):
    - I implemented Support Vector Machines from scratch using Python.
    - I used stocastic gradient decent optimization for my implementation of SVM.
    - My implementation can be found on GitHub [here](https://github.com/aakashsbhatia2/Support-Vector-Machine)
- K-means Clustering:
    - I implemented the k-means clustering algorithm from scratch using Python.
    - I used Euclidean distance, Manhattan distance and Minkowski distance metrics for my implementation
    - My implementation can be found on GitHub [here](https://github.com/aakashsbhatia2/k-means)
- K-nearest Neighbors:
    - I implemented the k nearest neighbors algorithm from scratch using Python 
    - My implementation can be found on GitHub [here](https://github.com/aakashsbhatia2/K-Nearest-Neighbors)


<hr /> 

<h3>COVID-19 Data Analysis using Hadoop and Spark</h3>
Used Hadoop and Spark to analyse a COVID-19 dataset. 
- I performed 3 tasks:
    - Obtain the number of cases per country and world-wide
    - Obtain the number of cases per country and world-wide for a given time period
    - Obtain country-wise number of cases per million
- Technologies used: [Hadoop](https://hadoop.apache.org/), [Spark](https://spark.apache.org/), Java
- My implementation can be found on GitHub [here](https://github.com/aakashsbhatia2/COVID-19-Analytics)